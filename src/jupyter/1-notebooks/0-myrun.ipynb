{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# myrun\n",
    "\n",
    "This notebook imports the feature vector, merges it with the labelled data and then makes predictions on specified columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from functools import reduce\n",
    "\n",
    "# Igel\n",
    "from igel import Igel\n",
    "\n",
    "# Compare Algorithms\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    df.replace(np.nan,0)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    df = df.replace(r'\\D+', '', regex=True)\n",
    "    return df\n",
    "    #return df[indices_to_keep].astype(np.float64)\n",
    "\n",
    "def clean_dataset_new(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)\n",
    "\n",
    "def clean_dataset_int(df):\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    df.replace(np.nan,0)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    df = df.replace(r'\\D+', '', regex=True)\n",
    "    return df[indices_to_keep].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the column names\n",
    "feature_vector_keys = [\"timestamp\", #0\n",
    "                       \"tweet_id\",  \n",
    "                       \"positive_sentiment\", \n",
    "                       \"negative_sentiment\", \n",
    "                       \"numb_of_mentions\", \n",
    "                       \"numb_of_media\",\n",
    "                       \"numb_of_urls\", \n",
    "                       \"numb_of_hashtags\", \n",
    "                       \"numb_of_personal_pronouns\", \n",
    "                       \"numb_of_present_tenses\", \n",
    "                       \"numb_of_past_tenses\", #10 \n",
    "                       #\"sent_from_web\",\n",
    "                       \"numb_of_named_entites\",\n",
    "                       \"numb_of_weird_chars\", \n",
    "                       \"numb_of_questions\", \n",
    "                       \"numb_of_emoticons\", \n",
    "                       \"numb_of_swearing_words\", \n",
    "                       \"numb_of_slang_words\", \n",
    "                       \"numb_of_intensifiers\", \n",
    "                       \"tweet_length\", \n",
    "                       \"userFollowersCount\",\n",
    "                       \"userFriendsCount\", #20\n",
    "                       \"user_numb_of_tweets\",\n",
    "                       \"user_list_count\",\n",
    "                       #\"tfidf_fire\",                      # old\n",
    "                       \"dict_precision\",                  # measures against the keyword TFIDF\n",
    "                       \"dict_recall\", \n",
    "                       \"dict_f_measure\",\n",
    "                       \"offset\",\n",
    "                       \"weighted_length\", # the overall length of the Tweet with code points weighted per the ranges\n",
    "                       \"permillage\", # (indicates the proportion (per thousand) of the weighted length in comparison to the max weighted length.\n",
    "                       \"is_verified\", # column 29  #30 Non-word-embedding-features (0-29)\n",
    "                       \"unknown1\",\n",
    "                       \"unknown2\",\n",
    "                       \"unknown3\",\n",
    "                       \"wEmbed1\",\n",
    "                       \"wEmbed2\",\n",
    "                       \"wEmbed3\",\n",
    "                       \"wEmbed4\",\n",
    "                       \"wEmbed5\",\n",
    "                       \"wEmbed6\",\n",
    "                       \"wEmbed7\",\n",
    "                       \"wEmbed8\",\n",
    "                       \"wEmbed9\",\n",
    "                       \"wEmbed10\",\n",
    "                       \"wEmbed11\",\n",
    "                       \"wEmbed12\",\n",
    "                       \"wEmbed13\",\n",
    "                       \"wEmbed14\",\n",
    "                       \"wEmbed15\",\n",
    "                       \"wEmbed16\",\n",
    "                       \"wEmbed17\",\n",
    "                       \"wEmbed18\",\n",
    "                       \"wEmbed19\",\n",
    "                       \"wEmbed20\",\n",
    "                       \"wEmbed21\",\n",
    "                       \"wEmbed22\",\n",
    "                       \"wEmbed23\",\n",
    "                       \"wEmbed24\",\n",
    "                       \"wEmbed25\",\n",
    "                       \"wEmbed26\",\n",
    "                       \"wEmbed27\",\n",
    "                       \"wEmbed28\",\n",
    "                       \"wEmbed29\",\n",
    "                       \"wEmbed30\",\n",
    "                       \"wEmbed31\",\n",
    "                       \"wEmbed32\",\n",
    "                       \"wEmbed33\",\n",
    "                       \"wEmbed34\",\n",
    "                       \"wEmbed35\",\n",
    "                       \"wEmbed36\",\n",
    "                       \"wEmbed37\",\n",
    "                       \"wEmbed38\",\n",
    "                       \"wEmbed39\",\n",
    "                       \"wEmbed40\",\n",
    "                       \"wEmbed41\",\n",
    "                       \"wEmbed42\",\n",
    "                       \"wEmbed43\",\n",
    "                       \"wEmbed44\",\n",
    "                       \"wEmbed45\",\n",
    "                       \"wEmbed46\",\n",
    "                       \"wEmbed47\",\n",
    "                       \"wEmbed48\",\n",
    "                       \"wEmbed49\",\n",
    "                       \"wEmbed51\",\n",
    "                       \"wEmbed52\",\n",
    "                       \"wEmbed53\",\n",
    "                       \"wEmbed54\",\n",
    "                       \"wEmbed55\",\n",
    "                       \"wEmbed56\",\n",
    "                       \"wEmbed57\",\n",
    "                       \"wEmbed58\",\n",
    "                       \"wEmbed59\",\n",
    "                       \"wEmbed60\",\n",
    "                       \"wEmbed61\",\n",
    "                       \"wEmbed62\",\n",
    "                       \"wEmbed63\",\n",
    "                       \"wEmbed64\",\n",
    "                       \"wEmbed65\",\n",
    "                       \"wEmbed66\",\n",
    "                       \"wEmbed67\",\n",
    "                       \"wEmbed68\",\n",
    "                       \"wEmbed69\",\n",
    "                       \"wEmbed70\",\n",
    "                       \"wEmbed71\",\n",
    "                       \"wEmbed72\",\n",
    "                       \"wEmbed73\",\n",
    "                       \"wEmbed74\",\n",
    "                       \"wEmbed75\",\n",
    "                       \"wEmbed76\",\n",
    "                       \"wEmbed77\",\n",
    "                       \"wEmbed78\",\n",
    "                       \"wEmbed79\",\n",
    "                       \"wEmbed80\",\n",
    "                       \"wEmbed81\",\n",
    "                       \"wEmbed82\",\n",
    "                       \"wEmbed83\",\n",
    "                       \"wEmbed84\",\n",
    "                       \"wEmbed85\",\n",
    "                       \"wEmbed86\",\n",
    "                       \"wEmbed87\",\n",
    "                       \"wEmbed88\",\n",
    "                       \"wEmbed89\",\n",
    "                       \"wEmbed80\",\n",
    "                       \"wEmbed91\",\n",
    "                       \"wEmbed92\",\n",
    "                       \"wEmbed93\",\n",
    "                       \"wEmbed94\",\n",
    "                       \"wEmbed95\",\n",
    "                       \"wEmbed96\",\n",
    "                       \"wEmbed97\",\n",
    "                       \"wEmbed98\",\n",
    "                       \"wEmbed99\",\n",
    "                       \"wEmbed100\"\n",
    "                       \"wEmbed101\",\n",
    "                       \"wEmbed102\",\n",
    "                       \"wEmbed103\",\n",
    "                       \"wEmbed104\",\n",
    "                       \"wEmbed105\",\n",
    "                       \"wEmbed106\",\n",
    "                       \"wEmbed107\",\n",
    "                       \"wEmbed108\",\n",
    "                       \"wEmbed109\",\n",
    "                       \"wEmbed110\",\n",
    "                       \"wEmbed111\",\n",
    "                       \"wEmbed112\",\n",
    "                       \"wEmbed113\",\n",
    "                       \"wEmbed114\",\n",
    "                       \"wEmbed115\",\n",
    "                       \"wEmbed116\",\n",
    "                       \"wEmbed117\",\n",
    "                       \"wEmbed118\",\n",
    "                       \"wEmbed119\",\n",
    "                       \"wEmbed120\",\n",
    "                       \"wEmbed121\",\n",
    "                       \"wEmbed122\",\n",
    "                       \"wEmbed123\",\n",
    "                       \"wEmbed124\",\n",
    "                       \"wEmbed125\",\n",
    "                       \"wEmbed126\",\n",
    "                       \"wEmbed127\",\n",
    "                       \"wEmbed128\",\n",
    "                       \"wEmbed129\",\n",
    "                       \"wEmbed130\",\n",
    "                       \"wEmbed131\",\n",
    "                       \"wEmbed132\",\n",
    "                       \"wEmbed133\",\n",
    "                       \"wEmbed134\",\n",
    "                       \"wEmbed135\",\n",
    "                       \"wEmbed136\",\n",
    "                       \"wEmbed137\",\n",
    "                       \"wEmbed138\",\n",
    "                       \"wEmbed139\",\n",
    "                       \"wEmbed140\",\n",
    "                       \"wEmbed141\",\n",
    "                       \"wEmbed142\",\n",
    "                       \"wEmbed143\",\n",
    "                       \"wEmbed144\",\n",
    "                       \"wEmbed145\",\n",
    "                       \"wEmbed146\",\n",
    "                       \"wEmbed147\",\n",
    "                       \"wEmbed148\",\n",
    "                       \"wEmbed149\",\n",
    "                       \"wEmbed151\",\n",
    "                       \"wEmbed152\",\n",
    "                       \"wEmbed153\",\n",
    "                       \"wEmbed154\",\n",
    "                       \"wEmbed155\",\n",
    "                       \"wEmbed156\",\n",
    "                       \"wEmbed157\",\n",
    "                       \"wEmbed158\",\n",
    "                       \"wEmbed159\",\n",
    "                       \"wEmbed160\",\n",
    "                       \"wEmbed161\",\n",
    "                       \"wEmbed162\",\n",
    "                       \"wEmbed163\",\n",
    "                       \"wEmbed164\",\n",
    "                       \"wEmbed165\",\n",
    "                       \"wEmbed166\",\n",
    "                       \"wEmbed167\",\n",
    "                       \"wEmbed168\",\n",
    "                       \"wEmbed169\",\n",
    "                       \"wEmbed170\",\n",
    "                       \"wEmbed171\",\n",
    "                       \"wEmbed172\",\n",
    "                       \"wEmbed173\",\n",
    "                       \"wEmbed174\",\n",
    "                       \"wEmbed175\",\n",
    "                       \"wEmbed176\",\n",
    "                       \"wEmbed177\",\n",
    "                       \"wEmbed178\",\n",
    "                       \"wEmbed179\",\n",
    "                       \"wEmbed180\",\n",
    "                       \"wEmbed181\",\n",
    "                       \"wEmbed182\",\n",
    "                       \"wEmbed183\",\n",
    "                       \"wEmbed184\",\n",
    "                       \"wEmbed185\",\n",
    "                       \"wEmbed186\",\n",
    "                       \"wEmbed187\",\n",
    "                       \"wEmbed188\",\n",
    "                       \"wEmbed189\",\n",
    "                       \"wEmbed180\",\n",
    "                       \"wEmbed191\",\n",
    "                       \"wEmbed192\",\n",
    "                       \"wEmbed193\",\n",
    "                       \"wEmbed194\",\n",
    "                       \"wEmbed195\",\n",
    "                       \"wEmbed196\",\n",
    "                       \"wEmbed197\",\n",
    "                       \"wEmbed198\",\n",
    "                       \"wEmbed199\",\n",
    "                       \"wEmbed200\"\n",
    "                       ]\n",
    "\n",
    "\n",
    "priority_scorer = {\n",
    "    '10' : 'Critical',\n",
    "    '9' : 'Critical',\n",
    "    '8' : 'Critical',\n",
    "    '7' : 'High',\n",
    "    '6' : 'High',\n",
    "    '5' : 'Medium',\n",
    "    '4' : 'Medium',\n",
    "    '3' : 'Low',\n",
    "    '2' : 'Low',\n",
    "    '1.0' : 'Low',\n",
    "    '0.0' : 'Low',\n",
    "}\n",
    "\n",
    "priority_mapping = {\n",
    "    \"Critical\" : 10,\n",
    "    \"High\" : 7.5,\n",
    "    \"Medium\" : 5,\n",
    "    \"Low\" : 2.5,\n",
    "    \"Unknown\" : 0,\n",
    "}\n",
    "\n",
    "# What we consider to be highly important categories of information\n",
    "highCategoriser = {\n",
    "    0.0 : 'Other-Advice',\n",
    "    1.0 : 'Other-Advice',\n",
    "    2.0 : 'Report-CleanUp',\n",
    "    3.0 : 'ContextualInformation',\n",
    "    4.0 : 'Other-ContextualInformation',\n",
    "    5.0 : 'CallToAction-Donations',\n",
    "    6.0 : 'Report-EmergingThreats',\n",
    "    7.0 : 'Report-Factoid',\n",
    "    8.0 : 'Report-FirstPartyObservation',\n",
    "    9.0 : 'Request-GoodsServices',\n",
    "    10.0 : 'Report-Hashtags',\n",
    "    11.0 : 'Request-InformationWanted',\n",
    "    12.0 : 'Other-Irrelevant',\n",
    "    13.0 : 'Report-Location',\n",
    "    14.0 : 'CallToAction-MovePeople',\n",
    "    15.0 : 'Report-MultimediaShare',\n",
    "    16.0 : 'Report-NewSubEvent',\n",
    "    17.0 : 'Report-News',\n",
    "    18.0 : 'Report-Official',\n",
    "    19.0 : 'Report-OriginalEvent',\n",
    "    20.0 : 'Request-SearchAndRescue',\n",
    "    21.0 : 'Other-Sentiment',\n",
    "    22.0 : 'Report-ServiceAvailable',\n",
    "    23.0 : 'Report-ThirdPartyObservation',\n",
    "    24.0 : 'CallToAction-Volunteer',\n",
    "    25.0 : 'Report-Weather',\n",
    "    26.0 : 'hmm',\n",
    "}\n",
    "\n",
    "event_int_map =\t{\n",
    "  \"guatemalaEarthquake2012\": 7,\n",
    "  \"joplinTornado2011\": 16,\n",
    "  \"athensEarthquake2020\": 35,\n",
    "  \"baltimoreFlashFlood2020\": 36,\n",
    "  \"brooklynBlockPartyShooting2020\": 37,\n",
    "  \"daytonOhioShooting2020\": 38,\n",
    "  \"elPasoWalmartShooting2020\": 39,\n",
    "  \"gilroygarlicShooting2020\": 40,\n",
    "  \"hurricaneBarry2020\": 41,\n",
    "  \"indonesiaEarthquake2020\": 42,\n",
    "  \"keralaFloods2020\": 43,\n",
    "  \"myanmarFloods2020\": 44,\n",
    "  \"papuaNewguineaEarthquake2020\": 45,\n",
    "  \"siberianWildfires2020\": 46,\n",
    "  \"typhoonKrosa2020\": 47,\n",
    "  \"typhoonLekima2020\": 48,\n",
    "  \"whaleyBridgeCollapse2020\": 49\n",
    "}\n",
    "\n",
    "mymap = {'Advice':1, 'CleanUp':2, 'ContextualInformation':3, 'Discussion':4, 'Donations':5, \n",
    "        'EmergingThreats':6, 'Factoid':7, 'FirstPartyObservation':8, 'GoodsServices':9, 'Hashtags':10, \n",
    "        'InformationWanted':11,'Irrelevant':12, 'Location':13, 'MovePeople':14, \n",
    "         'MultimediaShare':15, 'NewSubEvent':16, 'News':17,\n",
    "        'Official':18, 'OriginalEvent':19, 'SearchAndRescue':20, 'Sentiment':21, 'ServiceAvailable':22, \n",
    "         'ThirdPartyObservation':23,'Volunteer':24, 'Weather':25}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Vector\n",
    "\n",
    "Load the feature vector in from Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../0-data/processed/new_with_offset.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-602813200a8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Currently loading the feature vector from a .txt file exported from Play\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeature_vector_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../../0-data/processed/new_with_offset.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Remove superflous \"]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mfeature_vector_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m130\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1043\u001b[0m             )\n\u001b[1;32m   1044\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1862\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1863\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1864\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \"\"\"\n\u001b[0;32m-> 1357\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1358\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../0-data/processed/new_with_offset.txt'"
     ]
    }
   ],
   "source": [
    "# Currently loading the feature vector from a .txt file exported from Play\n",
    "feature_vector_input = pd.read_csv(\"../../../0-data/processed/new_with_offset.txt\", sep=\",\", header=None, error_bad_lines=False)\n",
    "\n",
    "# Remove superflous \"]\"\n",
    "del feature_vector_input[130]\n",
    "\n",
    "feature_vector_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the column names\n",
    "feature_vector_input.columns = feature_vector_keys\n",
    "\n",
    "\n",
    "\n",
    "feature_vector_input#.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by tweet\n",
    "feature_vector_input = feature_vector_input.groupby(['tweet_id']).agg('first')\n",
    "\n",
    "# Reset the index\n",
    "feature_vector_input.reset_index(level=0, inplace=True)\n",
    "\n",
    "# Create a numeric version for our model\n",
    "feature_vector_input = clean_dataset(feature_vector_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove superflous \"]\"\n",
    "del feature_vector_input[\"wEmbed200\"]\n",
    "\n",
    "# Create a numeric version for our model\n",
    "feature_vector_input = clean_dataset_int(feature_vector_input)\n",
    "\n",
    "feature_vector_input#.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the labelled data\n",
    "\n",
    "These are generated in 0_Labels.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypes needs to be specified or long ints will change\n",
    "labels_df = pd.read_json(\"../../../0-data/raw/data/2020/2020-A/labels/TRECIS-2018-2020A-labels.json\", dtype={} )\n",
    "\n",
    "# replace the event with a numeric value\n",
    "labels_df = labels_df.replace({'eventID': event_int_map})\n",
    "\n",
    "# Count the number of labels\n",
    "labels_df['num'] = labels_df['postCategories'].str.len()\n",
    "\n",
    "\n",
    "# Map the priority to a numeric value\n",
    "labels_df = labels_df.replace({\"postPriority\": priority_mapping})\n",
    "\n",
    "# Split categories and map to numeric values\n",
    "category_list = pd.DataFrame(labels_df[\"postCategories\"].to_list(), columns=['cat1', 'cat2', 'cat3',\n",
    "                                                                   'cat4', 'cat5', 'cat6',\n",
    "                                                                   'cat7', 'cat8', 'cat9', 'cat10'])\n",
    "\n",
    "\n",
    "# Map the categories to numeric values\n",
    "category_list = category_list.applymap(lambda s: mymap.get(s) if s in mymap else s)\n",
    "\n",
    "\n",
    "# Join back onto our original list\n",
    "labels = labels_df.join(category_list)\n",
    "\n",
    "# Drop the string categories\n",
    "labels.drop(['postCategories'], axis = 1, inplace = True)\n",
    "\n",
    "# Tidy\n",
    "labels = labels.drop(['eventName', 'eventDescription', 'eventType'], axis=1)\n",
    "\n",
    "\n",
    "# Fill the NaN slots with 0\n",
    "labels = labels.fillna(\"0\")\n",
    "\n",
    "# Export\n",
    "labels.to_csv(\"../labels.csv\", index=False)\n",
    "\n",
    "labels = clean_dataset_int(labels)\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train.csv\n",
    "\n",
    "\n",
    "Merges the input feature vector with the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfs = [labels, feature_vector_input]\n",
    "#train = reduce(lambda left,right: pd.merge(left = 'tweet_id', right = 'postID'), dfs)\n",
    "\n",
    "train = pd.merge(labels, feature_vector_input, left_on = 'postID', right_on = 'tweet_id', how = 'inner')\n",
    "\n",
    "train.to_csv(\"../train.csv\", index=False)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test.csv\n",
    "\n",
    "\n",
    "Drops the categories, number of categories and priority so we can make our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train\n",
    "\n",
    "# Drop cat*\n",
    "test.drop(list(test.filter(regex = 'cat')), axis = 1, inplace = True)\n",
    "\n",
    "# Drop priority / num (of labels)\n",
    "test.drop(['postPriority', 'num'], axis = 1, inplace = True)\n",
    "\n",
    "# export\n",
    "test.to_csv(\"../test.csv\", index=False)\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Igel\n",
    "\n",
    "This notebook implements `Igel`\n",
    "\n",
    "> Igel supports all sklearn's machine learning functionality,\n",
    "\n",
    "Igel's supported models:\n",
    "\n",
    "        +--------------------+----------------------------+-------------------------+\n",
    "        |      regression    |        classification      |        clustering       |\n",
    "        +--------------------+----------------------------+-------------------------+\n",
    "        |   LinearRegression |         LogisticRegression |                  KMeans |\n",
    "        |              Lasso |                      Ridge |     AffinityPropagation |\n",
    "        |          LassoLars |               DecisionTree |                   Birch |\n",
    "        | BayesianRegression |                  ExtraTree | AgglomerativeClustering |\n",
    "        |    HuberRegression |               RandomForest |    FeatureAgglomeration |\n",
    "        |              Ridge |                 ExtraTrees |                  DBSCAN |\n",
    "        |  PoissonRegression |                        SVM |         MiniBatchKMeans |\n",
    "        |      ARDRegression |                  LinearSVM |    SpectralBiclustering |\n",
    "        |  TweedieRegression |                      NuSVM |    SpectralCoclustering |\n",
    "        | TheilSenRegression |            NearestNeighbor |      SpectralClustering |\n",
    "        |    GammaRegression |              NeuralNetwork |               MeanShift |\n",
    "        |   RANSACRegression | PassiveAgressiveClassifier |                  OPTICS |\n",
    "        |       DecisionTree |                 Perceptron |                    ---- |\n",
    "        |          ExtraTree |               BernoulliRBM |                    ---- |\n",
    "        |       RandomForest |           BoltzmannMachine |                    ---- |\n",
    "        |         ExtraTrees |       CalibratedClassifier |                    ---- |\n",
    "        |                SVM |                   Adaboost |                    ---- |\n",
    "        |          LinearSVM |                    Bagging |                    ---- |\n",
    "        |              NuSVM |           GradientBoosting |                    ---- |\n",
    "        |    NearestNeighbor |        BernoulliNaiveBayes |                    ---- |\n",
    "        |      NeuralNetwork |      CategoricalNaiveBayes |                    ---- |\n",
    "        |         ElasticNet |       ComplementNaiveBayes |                    ---- |\n",
    "        |       BernoulliRBM |         GaussianNaiveBayes |                    ---- |\n",
    "        |   BoltzmannMachine |      MultinomialNaiveBayes |                    ---- |\n",
    "        |           Adaboost |                       ---- |                    ---- |\n",
    "        |            Bagging |                       ---- |                    ---- |\n",
    "        |   GradientBoosting |                       ---- |                    ---- |\n",
    "        +--------------------+----------------------------+-------------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'cmd': 'fit',    \n",
    "        'data_path': \"../train.csv\",\n",
    "        'yaml_path': 'yaml/multi.yaml'  # DecisionTree\n",
    "}\n",
    "\n",
    "Igel(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'cmd': 'evaluate',    \n",
    "        'data_path': \"../train.csv\",\n",
    "        'yaml_path': 'yaml/hyper.yaml'\n",
    "} \n",
    "Igel(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'cmd': 'predict',    \n",
    "        'data_path': \"../test.csv\",\n",
    "        'yaml_path': 'yaml/hyper.yaml'\n",
    "}\n",
    "Igel(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(\"model_results/predictions.csv\")\n",
    "predictions\n",
    "\n",
    "def is_neg_predictions(predictions):\n",
    "    predictions = predictions.sort_values(by=['postPriority'])\n",
    "    predictions = predictions[(predictions > 0).all(1)]\n",
    "    predictions.round()\n",
    "    \n",
    "#is_neg_predictions(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the new predictions back onto dataframe with the missing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the labels to their High Level Information Types\n",
    "cat_list = predictions.filter(regex='cat', axis=1).round().applymap(lambda x: highCategoriser[x])\n",
    "predictions = cat_list.combine_first(predictions)\n",
    "\n",
    "# Merge the predictions back into the training set\n",
    "df = test.merge(predictions, left_index=True, right_index=True)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Append the predicted categories to a list in a new column\n",
    "df['predicted_categories'] = df[['cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cat10']].values.tolist()\n",
    "\n",
    "\n",
    "# Get the number of categories into something we can use to index\n",
    "df['num'] = df['num'].astype(float).astype(int)\n",
    "\n",
    "# Remove categories beyond what the tweet is predicted to have\n",
    "df['categories'] = df.apply(lambda x: x['predicted_categories'][0:x['num']], axis=1)\n",
    "\n",
    "\n",
    "# Clean\n",
    "#df = df.filter(['eventID', 'tweet_id', 'postPriority', 'categories'], axis=1).round()\n",
    "\n",
    "#df = clean_dataset_int(df)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export\n",
    "\n",
    "Export in the TRECIS format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testy = pd.read_csv(\"../3-csv/testy.csv\")\n",
    "#testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to .run file\n",
    "with open(\"marks2.run\" , \"w\") as out_file:\n",
    "    for row in df.drop_duplicates(subset=\"tweet_id\").itertuples():\n",
    "        #print(\"row:\", row)\n",
    "        content = [\n",
    "            \"TRECIS-CTIT-H-Test-0\" + str(int(row.eventID)),\n",
    "            \"Q0\",          \n",
    "            np.int64(row.tweet_id),   \n",
    "            getattr(row, 'Index'),  #ToDo: Fix?\n",
    "            #row.priority,\n",
    "            str(priority_scorer[str(round(row.postPriority))[:3]]),  #ToDo: Fix\n",
    "            row.categories,\n",
    "            \"marksrun2\"\n",
    "        ]\n",
    "        out_file.write(\"\\t\".join([str(x) for x in content]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "rcParams['figure.figsize'] = 20,10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm Comparison Boxplot\n",
    "\n",
    "# load dataset\n",
    "array = train.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# prepare configuration for cross validation test harness\n",
    "seed = 7\n",
    "\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LogisticRegression', LogisticRegression()))\n",
    "models.append(('LinearDiscriminantAnalysis', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNeighborsClassifier', KNeighborsClassifier()))\n",
    "models.append(('DecisionTreeClassifier', DecisionTreeClassifier()))\n",
    "models.append(('GaussianNB', GaussianNB()))\n",
    "models.append(('SVC', SVC()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "#fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "#plt.boxplot(results)\n",
    "sns.boxplot(data=results, palette=\"Set3\")\n",
    "ax.set_xticklabels(names)\n",
    "plt.title('Comparison of Model by Classification Metric')\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('../../../0-data/screenshots/benchmark_models_performance.png',dpi=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example run\n",
    "# Run             & NDCG    &  AW-H     & AW-A      & CF1-H   & CF1-A   & CAcc   & PErr-H & PErr-A \\\\\n",
    "# njit-sub01.text & 0.4632  & -0.4801   & -0.2493   & 0.0792  & 0.1582  & 0.9025 & 0.1524 & 0.2198 \\\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
