{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install numpy\n",
    "#!pip3 install pandas\n",
    "#!pip3 install requests\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from functools import reduce\n",
    "\n",
    "# Igel\n",
    "from igel import Igel\n",
    "\n",
    "# Compare Algorithms\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    #assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    #df.dropna(inplace=True)\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    df.replace(np.nan,0)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    df = df.replace(r'\\D+', '', regex=True)\n",
    "    return df\n",
    "    #return df[indices_to_keep].astype(np.float64)\n",
    "    \n",
    "\n",
    "def clean_dataset_int(df):\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    df.replace(np.nan,0)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    df = df.replace(r'\\D+', '', regex=True)\n",
    "    return df[indices_to_keep].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_scorer = {\n",
    "    '10' : 'Critical',\n",
    "    '9' : 'Critical',\n",
    "    '8' : 'Critical',\n",
    "    '7' : 'High',\n",
    "    '6' : 'High',\n",
    "    '5' : 'Medium',\n",
    "    '4' : 'Medium',\n",
    "    '3' : 'Low',\n",
    "    '2' : 'Low',\n",
    "    '1.0' : 'Low',\n",
    "    '0.0' : 'Low',\n",
    "}\n",
    "\n",
    "priority_mapping = {\n",
    "    \"Critical\" : 10,\n",
    "    \"High\" : 7.5,\n",
    "    \"Medium\" : 5,\n",
    "    \"Low\" : 2.5,\n",
    "    \"Unknown\" : 0,\n",
    "}\n",
    "\n",
    "# What we consider to be highly important categories of information\n",
    "highCategoriser = {\n",
    "    0.0 : 'Other-Advice',\n",
    "    1.0 : 'Other-Advice',\n",
    "    2.0 : 'Report-CleanUp',\n",
    "    3.0 : 'ContextualInformation',\n",
    "    4.0 : 'Other-ContextualInformation',\n",
    "    5.0 : 'CallToAction-Donations',\n",
    "    6.0 : 'Report-EmergingThreats',\n",
    "    7.0 : 'Report-Factoid',\n",
    "    8.0 : 'Report-FirstPartyObservation',\n",
    "    9.0 : 'Request-GoodsServices',\n",
    "    10.0 : 'Report-Hashtags',\n",
    "    11.0 : 'Request-InformationWanted',\n",
    "    12.0 : 'Other-Irrelevant',\n",
    "    13.0 : 'Report-Location',\n",
    "    14.0 : 'CallToAction-MovePeople',\n",
    "    15.0 : 'Report-MultimediaShare',\n",
    "    16.0 : 'Report-NewSubEvent',\n",
    "    17.0 : 'Report-News',\n",
    "    18.0 : 'Report-Official',\n",
    "    19.0 : 'Report-OriginalEvent',\n",
    "    20.0 : 'Request-SearchAndRescue',\n",
    "    21.0 : 'Other-Sentiment',\n",
    "    22.0 : 'Report-ServiceAvailable',\n",
    "    23.0 : 'Report-ThirdPartyObservation',\n",
    "    24.0 : 'CallToAction-Volunteer',\n",
    "    25.0 : 'Report-Weather',\n",
    "    26.0 : 'hmm',\n",
    "}\n",
    "\n",
    "event_int_map =\t{\n",
    "  \"guatemalaEarthquake2012\": 7,\n",
    "  \"joplinTornado2011\": 16,\n",
    "  \"athensEarthquake2020\": 35,\n",
    "  \"baltimoreFlashFlood2020\": 36,\n",
    "  \"brooklynBlockPartyShooting2020\": 37,\n",
    "  \"daytonOhioShooting2020\": 38,\n",
    "  \"elPasoWalmartShooting2020\": 39,\n",
    "  \"gilroygarlicShooting2020\": 40,\n",
    "  \"hurricaneBarry2020\": 41,\n",
    "  \"indonesiaEarthquake2020\": 42,\n",
    "  \"keralaFloods2020\": 43,\n",
    "  \"myanmarFloods2020\": 44,\n",
    "  \"papuaNewguineaEarthquake2020\": 45,\n",
    "  \"siberianWildfires2020\": 46,\n",
    "  \"typhoonKrosa2020\": 47,\n",
    "  \"typhoonLekima2020\": 48,\n",
    "  \"whaleyBridgeCollapse2020\": 49\n",
    "}\n",
    "\n",
    "mymap = {'Advice':1, 'CleanUp':2, 'ContextualInformation':3, 'Discussion':4, 'Donations':5, \n",
    "        'EmergingThreats':6, 'Factoid':7, 'FirstPartyObservation':8, 'GoodsServices':9, 'Hashtags':10, \n",
    "        'InformationWanted':11,'Irrelevant':12, 'Location':13, 'MovePeople':14, \n",
    "         'MultimediaShare':15, 'NewSubEvent':16, 'News':17,\n",
    "        'Official':18, 'OriginalEvent':19, 'SearchAndRescue':20, 'Sentiment':21, 'ServiceAvailable':22, \n",
    "         'ThirdPartyObservation':23,'Volunteer':24, 'Weather':25}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Vector\n",
    "\n",
    "Load the feature vector in from Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 23944 column 1 (char 1093374)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ba1a3955b369>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://tweetminer-2336003gproject.ida.dcs.gla.ac.uk/stored_tweets'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    898\u001b[0m                     \u001b[0;31m# used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extra data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 23944 column 1 (char 1093374)"
     ]
    }
   ],
   "source": [
    "#url = 'http://localhost:9000/stored_tweets'\n",
    "import json\n",
    "def parse_json_stream(stream):\n",
    "    decoder = json.JSONDecoder()\n",
    "    while stream:\n",
    "        obj, idx = decoder.raw_decode(stream)\n",
    "        yield obj\n",
    "        stream = stream[idx:].lstrip()\n",
    "\n",
    "url = 'http://tweetminer-2336003gproject.ida.dcs.gla.ac.uk/stored_tweets'\n",
    "\n",
    "data = requests.get(url).json()\n",
    "\n",
    "\n",
    "\n",
    "q = requests.get(url)\n",
    "\n",
    "#json_response = json.loads(q.text.replace('ï¿½', ''))\n",
    "\n",
    "# JSONDecodeError: Expecting ',' delimiter: \n",
    "json_response = json.loads('[' + q.text + '],')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(df['tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df[df['sentiment'].apply(lambda x:pd.Series(x))]\n",
    "df_sentiment=pd.concat([df['sentiment'].str.split(',', expand=True)], axis=1, keys=\"s\")\n",
    "df_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.join(df_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embeddings'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embeddings'] = df['embeddings'].str.extract('(\\d+)', expand=False)\n",
    "df['embeddings'] = df['embeddings'].str[1:]\n",
    "df['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embeddings = pd.concat([df['embeddings'].str.split(',', expand=True)], axis=1, keys=\"e\")\n",
    "df_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.join(df_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emb_sent=df_sentiment.join(df_embeddings)\n",
    "df_emb_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.join(df_emb_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "df.to_csv(\"temp/api_input.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_input = pd.read_csv(\"temp/api_input.csv\")\n",
    "api_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the string categories\n",
    "api_input.drop(['tweet_text'], axis = 1, inplace = True)\n",
    "api_input.drop(['embeddings'], axis = 1, inplace = True)\n",
    "api_input.drop(['sentiment'], axis = 1, inplace = True)\n",
    "\n",
    "api_input.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by tweet\n",
    "feature_vector_input = api_input.groupby(['tweet_id']).agg('first')\n",
    "feature_vector_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the labelled data\n",
    "\n",
    "These are generated in 0_Labels.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypes needs to be specified or long ints will change\n",
    "labels_df = pd.read_json(\"../../../data/input/raw/data/2020/2020-A/labels/TRECIS-2018-2020A-labels.json\", dtype={} )\n",
    "\n",
    "# replace the event with a numeric value\n",
    "labels_df = labels_df.replace({'eventID': event_int_map})\n",
    "\n",
    "# Count the number of labels\n",
    "labels_df['num'] = labels_df['postCategories'].str.len()\n",
    "\n",
    "\n",
    "# Map the priority to a numeric value\n",
    "labels_df = labels_df.replace({\"postPriority\": priority_mapping})\n",
    "\n",
    "# Split categories and map to numeric values\n",
    "category_list = pd.DataFrame(labels_df[\"postCategories\"].to_list(), columns=['cat1', 'cat2', 'cat3',\n",
    "                                                                   'cat4', 'cat5', 'cat6',\n",
    "                                                                   'cat7', 'cat8', 'cat9', 'cat10'])\n",
    "\n",
    "\n",
    "# Map the categories to numeric values\n",
    "category_list = category_list.applymap(lambda s: mymap.get(s) if s in mymap else s)\n",
    "\n",
    "\n",
    "# Join back onto our original list\n",
    "labels = labels_df.join(category_list)\n",
    "\n",
    "# Drop the string categories\n",
    "labels.drop(['postCategories'], axis = 1, inplace = True)\n",
    "\n",
    "# Tidy\n",
    "labels = labels.drop(['eventName', 'eventDescription', 'eventType'], axis=1)\n",
    "\n",
    "\n",
    "# Fill the NaN slots with 0\n",
    "labels = labels.fillna(\"0\")\n",
    "\n",
    "# Export\n",
    "labels.to_csv(\"../labels.csv\", index=False)\n",
    "labels = clean_dataset_int(labels)\n",
    "#labels = clean_dataset_new(labels)\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train.csv\n",
    "\n",
    "\n",
    "Merges the input feature vector with the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(labels, feature_vector_input, left_on = 'postID', right_on = 'tweet_id', how = 'inner')\n",
    "\n",
    "train = train.reset_index()\n",
    "train.fillna(0, inplace=True)\n",
    "\n",
    "train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "train = train.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "\n",
    "train.to_csv(\"../train.csv\", index=False)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test.csv\n",
    "\n",
    "\n",
    "Drops the categories, number of categories and priority so we can make our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train\n",
    "\n",
    "# Drop cat*\n",
    "test.drop(list(test.filter(regex = 'cat\\d+$')), axis = 1, inplace = True)\n",
    "\n",
    "# Drop priority / num (of labels)\n",
    "test.drop(['postPriority', 'num'], axis = 1, inplace = True)\n",
    "\n",
    "# export\n",
    "test.to_csv(\"../test.csv\", index=False)\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Igel\n",
    "\n",
    "This notebook implements `Igel`\n",
    "\n",
    "> Igel supports all sklearn's machine learning functionality,\n",
    "\n",
    "Igel's supported models:\n",
    "\n",
    "        +--------------------+----------------------------+-------------------------+\n",
    "        |      regression    |        classification      |        clustering       |\n",
    "        +--------------------+----------------------------+-------------------------+\n",
    "        |   LinearRegression |         LogisticRegression |                  KMeans |\n",
    "        |              Lasso |                      Ridge |     AffinityPropagation |\n",
    "        |          LassoLars |               DecisionTree |                   Birch |\n",
    "        | BayesianRegression |                  ExtraTree | AgglomerativeClustering |\n",
    "        |    HuberRegression |               RandomForest |    FeatureAgglomeration |\n",
    "        |              Ridge |                 ExtraTrees |                  DBSCAN |\n",
    "        |  PoissonRegression |                        SVM |         MiniBatchKMeans |\n",
    "        |      ARDRegression |                  LinearSVM |    SpectralBiclustering |\n",
    "        |  TweedieRegression |                      NuSVM |    SpectralCoclustering |\n",
    "        | TheilSenRegression |            NearestNeighbor |      SpectralClustering |\n",
    "        |    GammaRegression |              NeuralNetwork |               MeanShift |\n",
    "        |   RANSACRegression | PassiveAgressiveClassifier |                  OPTICS |\n",
    "        |       DecisionTree |                 Perceptron |                    ---- |\n",
    "        |          ExtraTree |               BernoulliRBM |                    ---- |\n",
    "        |       RandomForest |           BoltzmannMachine |                    ---- |\n",
    "        |         ExtraTrees |       CalibratedClassifier |                    ---- |\n",
    "        |                SVM |                   Adaboost |                    ---- |\n",
    "        |          LinearSVM |                    Bagging |                    ---- |\n",
    "        |              NuSVM |           GradientBoosting |                    ---- |\n",
    "        |    NearestNeighbor |        BernoulliNaiveBayes |                    ---- |\n",
    "        |      NeuralNetwork |      CategoricalNaiveBayes |                    ---- |\n",
    "        |         ElasticNet |       ComplementNaiveBayes |                    ---- |\n",
    "        |       BernoulliRBM |         GaussianNaiveBayes |                    ---- |\n",
    "        |   BoltzmannMachine |      MultinomialNaiveBayes |                    ---- |\n",
    "        |           Adaboost |                       ---- |                    ---- |\n",
    "        |            Bagging |                       ---- |                    ---- |\n",
    "        |   GradientBoosting |                       ---- |                    ---- |\n",
    "        +--------------------+----------------------------+-------------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'cmd': 'fit',    \n",
    "        'data_path': \"../train.csv\",\n",
    "        'yaml_path': 'yaml/multi.yaml'  # DecisionTree\n",
    "}\n",
    "\n",
    "Igel(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'cmd': 'predict',    \n",
    "        'data_path': \"../test.csv\",\n",
    "        'yaml_path': 'yaml/hyper.yaml'\n",
    "}\n",
    "Igel(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "\n",
    "1. View the raw predictions\n",
    "2. Map the labels to their High Level Information Types\n",
    "3. Merge the predictions back into the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_neg_predictions(predictions):\n",
    "    predictions = predictions.sort_values(by=['postPriority'])\n",
    "    predictions = predictions[(predictions > 0).all(1)]\n",
    "    predictions.round()\n",
    "\n",
    "    \n",
    "predictions = pd.read_csv(\"model_results/predictions.csv\")\n",
    "predictions\n",
    "\n",
    "#is_neg_predictions(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the new predictions back onto dataframe with the missing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the labels to their High Level Information Types\n",
    "cat_list = predictions.filter(regex='cat', axis=1).round().applymap(lambda x: highCategoriser[x])\n",
    "\n",
    "#\n",
    "predictions = cat_list.combine_first(predictions)\n",
    "\n",
    "# Merge the predictions back into the training set\n",
    "df = test.merge(predictions, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "# Append the predicted categories to a list in a new column\n",
    "df['predicted_categories'] = df[['cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cat10']].values.tolist()\n",
    "\n",
    "\n",
    "# Get the number of categories into something we can use to index\n",
    "df['num'] = df['num'].astype(float).astype(int)\n",
    "\n",
    "# Remove categories beyond what the tweet is predicted to have\n",
    "df['categories'] = df.apply(lambda x: x['predicted_categories'][0:x['num']], axis=1)\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export\n",
    "\n",
    "Export in the TRECIS format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to .run file\n",
    "with open(\"marks2.run\" , \"w\") as out_file:\n",
    "    for row in df.drop_duplicates(subset=\"postID\").itertuples():\n",
    "        #print(\"row:\", row)\n",
    "        content = [\n",
    "            \"TRECIS-CTIT-H-Test-0\" + str(int(row.eventID)),\n",
    "            \"Q0\",          \n",
    "            np.int64(row.postID),   \n",
    "            getattr(row, 'Index'),  #ToDo: Fix?\n",
    "            #row.priority,\n",
    "            str(priority_scorer[str(round(row.postPriority))[:3]]),  #ToDo: Fix\n",
    "            row.categories,\n",
    "            \"marksrun2\"\n",
    "        ]\n",
    "        out_file.write(\"\\t\".join([str(x) for x in content]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
