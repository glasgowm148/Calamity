
\subsubsection{\uppercase{Twitter LDA}}
An issue pertaining to the use of LDA on Twitter data is the questionable assumption of considering a tweet as a mixture of topics. Given the extremely short nature of tweets, most tweets consists of a single topic. As discussed above, some studies have tackled this problem by aggregating the tweets of a user in a single document. This method, though effective, is not guaranteed to help much because of the fact that users generally express a wide variety of different topics in their tweets which may not be related to each other. This analysis will still be good if we want to profile users; but since our aim is to mine events from the topics, the possible solutions that seems feasible will be to aggregate tweets based on time, locality and hashtags.

To this end, \cite{zhao2011comparing} have proposed an effective variant of the standard LDA, called Twitter LDA. It is based on the assumption that a tweet will contain a single topic chosen from a topic distribution of a particular user. 

\paragraph{Model Description} 
The generative model makes the following assumptions. Twitter data has T number of topics. When a user tweets, he/she selects a topic from his/her favorite list of topics, these topics will be from the T topics. Then for the selected topic, the user selects a bag of words, one by one from the distribution of words over topics. However, all the words of the tweet need not necessarily describe the topic. Many of them are just common words occurring in tweets of various different topics. So for each of the words, user first decides whether the given word is closely describes the topic or not. Depending upon this the word will be classified as a topic word or a background word. He then chooses the word from its respective distribution.

Formally, let $\theta_u$ denotes the topic distribution for a user $u$. Let $\phi_t$ be the distribution of words for the topic $t$ and let $\phi_B$ be the distribution of background words. $\pi$ is a Bernoulli distribution which denote the word belongs to the background class or to the topic class. $\alpha,~\beta,~\gamma$ are Dirichlet parameter used for generating respective Dirichlet distributions. The plate notation for the model is given in Fig.\ref{fig:plate}. The generative algorithm is given in Fig.\ref{fig:twitterlda-algo}.

In addition to other LDA variants, {\bf Harshil Lodhi} has particularly focused on exploring Twitter-LDA.

