\section{\uppercase{Experiments}}
Parallel to our literature survey on topic detection techniques for Twitter, we have also conducted preliminary experiments on a Twitter dataset having 1.6 million tweets. We processed the available data which was in CSV format into a format that can be fed into the LDA program. Next, we ran LDA on this dataset, with each tweet as a document, without any tweet pooling or aggregation. The topic clusters (Table \ref{table1}) returned by LDA were noisy, with hashtags, usernames, slangs and keywords from all domains (stop words, spam words, mood-related etc.) cluttered together. Since the frequency of stop words far outnumbers that of interesting keywords which can potentially associate with an event, the top words in each topic are stop words only, and not words of interest. Another reason for these impure clusters was the fact that users use different and often incorrect spellings for the same words, resulting in potential assignment of same terms to different topics. Compare this to the topics (Table \ref{table2}) that were generated when we ran LDA on news data obtained from The American Political Science Review. One can see that the topic clusters are making a lot of sense in this case.

Our preliminary experiments have helped us gain better insight into the topic modeling paradigm, and the issues pertinent to its naive use over Twitter dataset. We are in the process of performing preprocessing on Twitter dataset to reduce noise, and experimenting with different variations of LDA to come up with a suitable model which generates meaningful and pure topic clusters.

The experiments have been conducted by {\bf Shobhit Chaurasia} using the code written by {\bf Harshil Lodhi} for processing and converting Twitter data from CSV format into the format compatible with the LDA code. The analysis of the results, and the observations thereof have been arrived at by {\bf all the three members}.
