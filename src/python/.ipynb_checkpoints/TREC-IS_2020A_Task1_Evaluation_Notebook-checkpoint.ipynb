{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading TRECIS2020A-t12-assr1.json\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'TRECIS2020A-t12-assr1.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0a7b3cfde127>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgroundtruthFile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclassificationLabelFiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reading \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mgroundtruthFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroundtruthFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'iso-8859-1'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgroundtruthJSONFile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mgroundtruthJSON\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroundtruthJSONFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;31m#pprint(groundtruthJSON[\"events\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'TRECIS2020A-t12-assr1.json'"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# TREC IS 2020 Evaluation Script\n",
    "# Configured for 2020-A Events\n",
    "# Task 1\n",
    "# Used to evaluate TREC-IS runs\n",
    "# --------------------------------------------------\n",
    "version = 2.4 # Notebook Version Number\n",
    "\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "runFile = \"_baseline.low.2020A.task1.run\"\n",
    "\n",
    "# Configuration (Change this to match your setting)\n",
    "# System output file to evaluate:\n",
    "# runFile = \"cbnuC1\"\n",
    "# runName = \"cbnuC1\"\n",
    "runName = \"_baseline.low\"\n",
    "\n",
    "# Do we try and normalize the run priority scores?\n",
    "enablePriorityNorm = False\n",
    "\n",
    "# The location of the ground truth data against which to compare the run\n",
    "classificationLabelFiles = [\n",
    "    \"TRECIS2020A-t12-assr1.json\",\n",
    "    \"TRECIS2020A-t12-assr2.json\",\n",
    "    \"TRECIS2020A-t12-assr3.json\",\n",
    "    \"TRECIS2020A-t12-assr4.json\",\n",
    "    \"TRECIS2020A-t12-assr5.json\"\n",
    "]\n",
    "classificationLabelFiles = [x for x in classificationLabelFiles]\n",
    "\n",
    "# The location of the ontology file\n",
    "ontologyFile = \"TRECIS-2020-ITypes-Task1.json\"\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Static data for the 2019 edition\n",
    "# --------------------------------------------------\n",
    "# Identifiers for the test events\n",
    "eventIdentifiers = [\n",
    "    \"athensEarthquake2020\",\n",
    "    \"baltimoreFlashFlood2020\",\n",
    "    \"brooklynBlockPartyShooting2020\",\n",
    "    \"daytonOhioShooting2020\",\n",
    "    \"elPasoWalmartShooting2020\",\n",
    "    \"gilroygarlicShooting2020\",\n",
    "    \"hurricaneBarry2020\",\n",
    "    \"indonesiaEarthquake2020\",\n",
    "    \"keralaFloods2020\",\n",
    "    \"myanmarFloods2020\",\n",
    "    \"papuaNewguineaEarthquake2020\",\n",
    "    \"siberianWildfires2020\",\n",
    "    \"typhoonKrosa2020\",\n",
    "    \"typhoonLekima2020\",\n",
    "    \"whaleyBridgeCollapse2020\"\n",
    "]\n",
    "\n",
    "eventidTopicidMap =\t{\n",
    "  \"athensEarthquake2020\": \"TRECIS-CTIT-H-Test-035\",\n",
    "  \"baltimoreFlashFlood2020\": \"TRECIS-CTIT-H-Test-036\",\n",
    "  \"brooklynBlockPartyShooting2020\": \"TRECIS-CTIT-H-Test-037\",\n",
    "  \"daytonOhioShooting2020\": \"TRECIS-CTIT-H-Test-038\",\n",
    "  \"elPasoWalmartShooting2020\": \"TRECIS-CTIT-H-Test-039\",\n",
    "  \"gilroygarlicShooting2020\": \"TRECIS-CTIT-H-Test-040\",\n",
    "  \"hurricaneBarry2020\": \"TRECIS-CTIT-H-Test-041\",\n",
    "  \"indonesiaEarthquake2020\": \"TRECIS-CTIT-H-Test-042\",\n",
    "  \"keralaFloods2020\": \"TRECIS-CTIT-H-Test-043\",\n",
    "  \"myanmarFloods2020\": \"TRECIS-CTIT-H-Test-044\",\n",
    "  \"papuaNewguineaEarthquake2020\": \"TRECIS-CTIT-H-Test-045\",\n",
    "  \"siberianWildfires2020\": \"TRECIS-CTIT-H-Test-046\",\n",
    "  \"typhoonKrosa2020\": \"TRECIS-CTIT-H-Test-047\",\n",
    "  \"typhoonLekima2020\": \"TRECIS-CTIT-H-Test-048\",\n",
    "  \"whaleyBridgeCollapse2020\": \"TRECIS-CTIT-H-Test-049\"\n",
    "}\n",
    "\n",
    "# What we consider to be highly important categories of information\n",
    "highImportCategories = [\n",
    "    \"Request-GoodsServices\",\n",
    "    \"Request-SearchAndRescue\",\n",
    "    \"CallToAction-MovePeople\",\n",
    "    \"Report-EmergingThreats\",\n",
    "    \"Report-NewSubEvent\",\n",
    "    \"Report-ServiceAvailable\"\n",
    "]\n",
    "\n",
    "highImportCategoriesShort = [\n",
    "    \"GoodsServices\",\n",
    "    \"SearchAndRescue\",\n",
    "    \"MovePeople\",\n",
    "    \"EmergingThreats\",\n",
    "    \"NewSubEvent\",\n",
    "    \"ServiceAvailable\"\n",
    "]\n",
    "\n",
    "# Parameters\n",
    "var_lambda = 0.75 # weight to place on actionable information categories in comparison to non actionable categoriee\n",
    "var_alpha = 0.3 # Flat gain for providing a correct alert, regardless of the categories selected\n",
    "\n",
    "resultsFile = open(runName+\".results.task1.overall.txt\",\"w+\")\n",
    "resultsFile.write(\"TREC-IS 2020-A Task 1 Notebook Evaluator v\"+str(version)+\"\\n\")\n",
    "resultsFile.write(\"Run: \"+runName+\" (\"+runFile+\")\"+\"\\n\")\n",
    "resultsFile.write(\"\"+\"\\n\")\n",
    "\n",
    "perTopicFile = open(runName+\".results.task1.pertopic.txt\",\"w+\")\n",
    "perTopicFile.write(\"TREC-IS 2020-A Task 1 Notebook Evaluator v\"+str(version)+\"\\n\")\n",
    "perTopicFile.write(\"Run: \"+runName+\" (\"+runFile+\")\"+\"\\n\")\n",
    "perTopicFile.write(\"\"+\"\\n\")\n",
    "\n",
    "perEventFile = open(runName+\".results.task1.perevent.txt\",\"w+\")\n",
    "perEventFile.write(\"TREC-IS 2020-A Task 1 Notebook Evaluator v\"+str(version)+\"\\n\")\n",
    "perEventFile.write(\"Run: \"+runName+\" (\"+runFile+\")\"+\"\\n\")\n",
    "perEventFile.write(\"\"+\"\\n\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Processing Starts Here\n",
    "# --------------------------------------------------\n",
    "import json\n",
    "from pprint import pprint\n",
    "import gzip\n",
    "import math\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Stage 1: Load the ground truth dataset \n",
    "# --------------------------------------------------\n",
    "\n",
    "groundtruthJSON = []\n",
    "for groundtruthFile in classificationLabelFiles:\n",
    "    print(\"Reading \"+groundtruthFile)\n",
    "    with open(groundtruthFile, encoding='iso-8859-1') as groundtruthJSONFile:    \n",
    "        groundtruthJSON.append(json.load(groundtruthJSONFile))\n",
    "#pprint(groundtruthJSON[\"events\"])\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Stage 2: Load run file \n",
    "# --------------------------------------------------\n",
    "with open(runFile, encoding='utf-8') as openRunFile:\n",
    "    runContents = openRunFile.readlines() # lines not yet decoded\n",
    "#pprint(runContents[0])\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Stage 3: Load the categories \n",
    "# --------------------------------------------------\n",
    "with open(ontologyFile, encoding='utf-8') as ontologyJSONFile:    \n",
    "    ontologyJSON = json.load(ontologyJSONFile)\n",
    "\n",
    "informationTypes2Index = {} # category -> numerical index\n",
    "informationTypesShort2Index = {} # category short form (e.g. Report-EmergingThreats vs. EmergingThreats) -> numerical index\n",
    "informationTypeIndex = 0\n",
    "for informationTypeJSON in ontologyJSON[\"informationTypes\"]:\n",
    "    informationTypeId = informationTypeJSON[\"id\"]\n",
    "    informationTypes2Index[informationTypeId] = informationTypeIndex\n",
    "    informationTypesShort2Index[informationTypeId.split(\"-\")[1]] = informationTypeIndex\n",
    "    informationTypeIndex = informationTypeIndex + 1\n",
    "    \n",
    "# -----------------------------------------------------------\n",
    "# Stage 4: Produce ground truth maps between tweetIds and categories\n",
    "# -----------------------------------------------------------\n",
    "# Notes: Ground truth is used as a base, if a run includes tweets\n",
    "#        not in the ground truth they will be ignored\n",
    "# Assumptions: A tweet will not be returned for multiple events\n",
    "\n",
    "tweetId2TRECInfoCategories = {} # tweet id -> Array of categories selected by assessors\n",
    "tweetId2TRECHighImportInfoCategories = {} # tweet id -> Array of categories selected by assessors\n",
    "tweetId2TRECLowImportInfoCategories = {} # tweet id -> Array of categories selected by assessors\n",
    "tweetId2TRECPriorityCategory = {} # tweet id -> priority label (Critical,High,Medium,Low)\n",
    "index2TweetId = {} # ordered tweets\n",
    "event2tweetIds = {} # event -> tweet ids for tweets within that event\n",
    "countHighCriticalImport = 0\n",
    "countLowMediumImport = 0\n",
    "\n",
    "tweetIndex = 0\n",
    "for groundtruth in groundtruthJSON:\n",
    "    for eventJSON in groundtruth[\"events\"]:\n",
    "        eventid = eventJSON[\"eventid\"]\n",
    "        #print(eventid)\n",
    "        # two events were split and assessed in parts, re-name these so they are correctly read\n",
    "        if eventid.endswith(\"A\") | eventid.endswith(\"B\") | eventid.endswith(\"C\") | eventid.endswith(\"D\") | eventid.endswith(\"E\"):\n",
    "            eventid = eventid[:-1]\n",
    "        \n",
    "        if not event2tweetIds.get(eventid):\n",
    "            event2tweetIds[eventid] = []\n",
    "        \n",
    "        if any(eventid in s for s in eventIdentifiers):\n",
    "            # iterate over tweets in the event\n",
    "            for tweetJSON in eventJSON[\"tweets\"]:\n",
    "                tweetid = tweetJSON[\"postID\"]\n",
    "                categories = tweetJSON[\"categories\"]\n",
    "                priority = tweetJSON[\"priority\"]\n",
    "                \n",
    "                if priority == \"High\" or priority == \"Critical\":\n",
    "                    countHighCriticalImport = countHighCriticalImport + 1\n",
    "                \n",
    "                if priority == \"Low\" or priority == \"Medium\":\n",
    "                    countLowMediumImport = countLowMediumImport + 1\n",
    "                \n",
    "                event2tweetIds[eventid].append(tweetid)\n",
    "                \n",
    "                # check categories for name issues and correct if possible\n",
    "                cleanedCategories = []\n",
    "                highImportCats = []\n",
    "                lowImportCats = []\n",
    "                for categoryId in categories:\n",
    "                    if not any(categoryId in s for s in informationTypesShort2Index.keys()):\n",
    "                        print(\"Found unknown category \"+categoryId)\n",
    "                    else:\n",
    "                        cleanedCategories.append(categoryId)\n",
    "                        if any(categoryId in s for s in highImportCategoriesShort):\n",
    "                            highImportCats.append(categoryId)\n",
    "                        else:\n",
    "                            lowImportCats.append(categoryId)\n",
    "    \n",
    "                tweetId2TRECInfoCategories[tweetid] = cleanedCategories\n",
    "                tweetId2TRECHighImportInfoCategories[tweetid] = highImportCats\n",
    "                tweetId2TRECLowImportInfoCategories[tweetid] = lowImportCats\n",
    "                tweetId2TRECPriorityCategory[tweetid] = priority\n",
    "                index2TweetId[tweetIndex] = tweetid;\n",
    "                tweetIndex = tweetIndex + 1\n",
    "\n",
    "                \n",
    "        else:\n",
    "            print(\"WARN: Found ground truth data for event not in the 2019 topic set \"+eventid+\", ignoring...\")\n",
    "        \n",
    "# -----------------------------------------------------------\n",
    "# Stage 5: Produce run predicted maps between tweetIds and categories\n",
    "# -----------------------------------------------------------\n",
    "tweetId2RunInfoCategories = {} # tweet id -> predicted category by participant system\n",
    "tweetId2RunHighImportInfoCategories = {} # tweet id -> predicted category by participant system\n",
    "tweetId2RunLowImportInfoCategories = {} # tweet id -> predicted category by participant system\n",
    "tweetId2RunPriorityScore = {} # tweet id -> importance score from participant system\n",
    "tweetId2RunPriorityScoreNorm = {} # tweet id -> importance score from participant system\n",
    "event2TweetIdRank = {} # event -> (rank,tweetid)\n",
    "\n",
    "maxPrediction = -999999\n",
    "minPrediction = 999999\n",
    "\n",
    "for runLine in runContents:\n",
    "    \n",
    "    for key in eventidTopicidMap:\n",
    "          runLine = runLine.replace(key, eventidTopicidMap[key])  \n",
    "    \n",
    "    #print(runLine)\n",
    "    \n",
    "    predictionParts = runLine.strip().replace(\", \", \",\").replace(\"\\\",\\\" \", \"\\\",\\\"\").replace(\" \",\"\\t\").replace(\"'\",\"\\\"\").split('\\t')\n",
    "    \n",
    "    #print(runLine)\n",
    "    if (len(predictionParts)<6 ):\n",
    "        print(runLine)\n",
    "        continue\n",
    "    else:\n",
    "        eventId = predictionParts[0]\n",
    "        tweetId = predictionParts[2]\n",
    "        rank = float(predictionParts[3])\n",
    "        #print(predictionParts[5])\n",
    "        categories = json.loads(predictionParts[5])\n",
    "        priority = predictionParts[4]\n",
    "        \n",
    "        tweetId2RunInfoCategories[tweetId] = categories\n",
    "        tweetId2RunPriorityScore[tweetId] = priority\n",
    "        \n",
    "        if not event2TweetIdRank.get(eventId):\n",
    "            event2TweetIdRank[eventId] = []\n",
    "        rankTuple = (tweetId,rank)\n",
    "        event2TweetIdRank.get(eventId).append(rankTuple)\n",
    "        \n",
    "        highImportCats = []\n",
    "        lowImportCats = []\n",
    "        for categoryId in categories:\n",
    "            cleanedCategories.append(categoryId)\n",
    "            if any(categoryId in s for s in highImportCategories):\n",
    "                highImportCats.append(categoryId)\n",
    "            else:\n",
    "                lowImportCats.append(categoryId)\n",
    "                \n",
    "        tweetId2RunHighImportInfoCategories[tweetId] = highImportCats\n",
    "        tweetId2RunLowImportInfoCategories[tweetId] = lowImportCats\n",
    "\n",
    "for eventId in event2TweetIdRank.keys():\n",
    "    tweetsSorted = sorted(event2TweetIdRank.get(eventId), key=lambda tup: tup[1])\n",
    "    event2TweetIdRank[eventId] = tweetsSorted\n",
    "    \n",
    "for i in range(len(index2TweetId)):\n",
    "    tweetId = index2TweetId[i]\n",
    "    if tweetId2RunPriorityScore.get(tweetId):\n",
    "        \n",
    "        # we used to do score normalization here, but no longer as priorites are now\n",
    "        # expressed by participants as categories\n",
    "        tweetId2RunPriorityScoreNorm[tweetId] = tweetId2RunPriorityScore.get(tweetId)\n",
    "        \n",
    "    else:\n",
    "        tweetId2RunPriorityScoreNorm[tweetId] = \"Low\"\n",
    "    \n",
    "# --------------------------------------------------\n",
    "# Stage 6: Create ground truth vectors per category\n",
    "# --------------------------------------------------\n",
    "\n",
    "category2GroundTruth = {} # category -> tweet vector with binary 1 vs all ground truth category labels\n",
    "\n",
    "for categoryId in informationTypes2Index.keys():\n",
    "    categoryIdShort = categoryId.split(\"-\")[1]\n",
    "    categoryVector = []\n",
    "    for i in range(len(index2TweetId)):\n",
    "        tweetId = index2TweetId[i]\n",
    "        categories = tweetId2TRECInfoCategories.get(tweetId)\n",
    "        #pprint(categories)\n",
    "        if any(categoryIdShort in s for s in categories):\n",
    "            categoryVector.append(1)\n",
    "        else:\n",
    "            categoryVector.append(0)\n",
    "    category2GroundTruth[categoryId] = categoryVector\n",
    "            \n",
    "#pprint(category2GroundTruth)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Stage 7: Create run vectors per category \n",
    "# --------------------------------------------------\n",
    "# Assumptions: If run misses a tweet, we assume it has\n",
    "#              no categories\n",
    "category2Predicted = {} # category -> tweet vector with binary 1 vs all predicted by system labels\n",
    "\n",
    "for categoryId in informationTypes2Index.keys():\n",
    "    categoryIdShort = categoryId.split(\"-\")[1]\n",
    "    categoryVector = []\n",
    "    for i in range(len(index2TweetId)):\n",
    "        tweetId = index2TweetId[i]\n",
    "        \n",
    "        if tweetId2RunInfoCategories.get(tweetId):\n",
    "            categories = tweetId2RunInfoCategories.get(tweetId)\n",
    "            if any(categoryIdShort in s for s in categories):\n",
    "                categoryVector.append(1)\n",
    "            else:\n",
    "                categoryVector.append(0)\n",
    "        else:\n",
    "            categoryVector.append(0)\n",
    "\n",
    "    category2Predicted[categoryId] = categoryVector\n",
    "\n",
    "#pprint(category2Predicted)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Stage 8: Make event category vectors \n",
    "# --------------------------------------------------\n",
    "\n",
    "event2groundtruth = {} # event -> category -> tweet vector with binary 1 vs all ground truth category labels\n",
    "for eventId in eventIdentifiers:\n",
    "    eventCategories = {}\n",
    "    for categoryId in informationTypes2Index.keys():\n",
    "        categoryIdShort = categoryId.split(\"-\")[1]\n",
    "        categoryVector = []\n",
    "        #print(eventId)\n",
    "        for tweetId in event2tweetIds.get(eventId):\n",
    "            #print(tweetId)\n",
    "            categories = tweetId2TRECInfoCategories.get(tweetId)\n",
    "            if any(categoryIdShort in s for s in categories):\n",
    "                categoryVector.append(1)\n",
    "            else:\n",
    "                categoryVector.append(0)\n",
    "            \n",
    "        eventCategories[categoryId] = categoryVector\n",
    "    event2groundtruth[eventId] = eventCategories\n",
    "    \n",
    "\n",
    "event2prediction = {} # event -> category -> tweet vector with binary 1 vs all predicted by system labels\n",
    "for eventId in eventIdentifiers:\n",
    "    eventCategories = {}\n",
    "    for categoryId in informationTypes2Index.keys():\n",
    "        categoryIdShort = categoryId.split(\"-\")[1]\n",
    "        categoryVector = []\n",
    "        #print(tweetId)\n",
    "        for tweetId in event2tweetIds.get(eventId):\n",
    "            #print(tweetId)\n",
    "            categories = tweetId2RunInfoCategories.get(tweetId)\n",
    "            \n",
    "            if categories == None:\n",
    "                categories = json.loads(\"[]\")\n",
    "                tweetId2RunInfoCategories[tweetId] = categories\n",
    "            \n",
    "            if any(categoryId in s for s in categories):\n",
    "                categoryVector.append(1)\n",
    "            else:\n",
    "                categoryVector.append(0)\n",
    "            \n",
    "        eventCategories[categoryId] = categoryVector\n",
    "    event2prediction[eventId] = eventCategories\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Stage 9: Make priority classification vectors\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "category2GroundTruthPriority = {} # category -> tweet vector with binary 1 vs all ground truth priority labels\n",
    "\n",
    "for categoryId in informationTypes2Index.keys():\n",
    "    categoryIdShort = categoryId.split(\"-\")[1]\n",
    "    priorityVector = []\n",
    "    for i in range(len(index2TweetId)):\n",
    "        tweetId = index2TweetId[i]\n",
    "        categories = tweetId2TRECInfoCategories.get(tweetId)\n",
    "        if any(categoryIdShort in s for s in categories):\n",
    "            priority = tweetId2TRECPriorityCategory.get(tweetId)\n",
    "            priorityVector.append(priority)\n",
    "    category2GroundTruthPriority[categoryId] = priorityVector\n",
    "\n",
    "category2PredictedPriority = {} # category -> tweet vector with binary 1 vs all predicted by system labels\n",
    "\n",
    "for categoryId in informationTypes2Index.keys():\n",
    "    categoryIdShort = categoryId.split(\"-\")[1]\n",
    "    categoryVector = []\n",
    "    for i in range(len(index2TweetId)):\n",
    "        tweetId = index2TweetId[i]\n",
    "        categories = tweetId2TRECInfoCategories.get(tweetId)\n",
    "        if any(categoryIdShort in s for s in categories):\n",
    "            if tweetId2RunPriorityScore.get(tweetId):\n",
    "                priority = tweetId2RunPriorityScore.get(tweetId)\n",
    "            \n",
    "                categoryVector.append(priority)\n",
    "            else:\n",
    "                categoryVector.append(\"Low\") # default to low priority\n",
    "\n",
    "    category2PredictedPriority[categoryId] = categoryVector\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Disable Warnings (comment this out when debugging!)\n",
    "# --------------------------------------------------\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # ignore warnings about 0-score categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# TREC-IS 2020-A\n",
    "# Task 1\n",
    "# Mertic: Accumulated Alert Worth\n",
    "# Measures system effectiveness from the perspective\n",
    "# of end-user alerting of important information\n",
    "# --------------------------------------------------\n",
    "\n",
    "totalHighImportWorth = 0.0\n",
    "totalLowImportWorth = 0.0\n",
    "AccumulatedAlertWorth = 0.0\n",
    "\n",
    "\n",
    "for eventId in event2TweetIdRank.keys():\n",
    "    numConsecutiveFalseAlerts = 0\n",
    "    for tweetIdRank in event2TweetIdRank[eventId]:\n",
    "        \n",
    "        tweetId = tweetIdRank[0]\n",
    "        try:\n",
    "            trecPriority = tweetId2TRECPriorityCategory[tweetId]\n",
    "        except:\n",
    "            #print(tweetId)\n",
    "            continue # skip tweets not in the ground truth\n",
    "        runPriority = tweetId2RunPriorityScoreNorm[tweetId]\n",
    "        \n",
    "        trecHighImportCats = set(tweetId2TRECHighImportInfoCategories[tweetid])\n",
    "        trecLowImportCats = set(tweetId2TRECHighImportInfoCategories[tweetid])\n",
    "        runHighImportCats = set(tweetId2RunHighImportInfoCategories[tweetId])\n",
    "        runLowImportCats = set(tweetId2RunLowImportInfoCategories[tweetId])\n",
    "        \n",
    "        gamma = 0\n",
    "        if len(trecHighImportCats)>0:\n",
    "            gamma = var_lambda\n",
    "        \n",
    "        ActCScore = 0.0\n",
    "        if len(trecHighImportCats | runHighImportCats) > 0: ActCScore = gamma*(len(trecHighImportCats & runHighImportCats) / len(trecHighImportCats | runHighImportCats))\n",
    "        NActCScore = 0.0\n",
    "        if len(trecLowImportCats | runLowImportCats)>0: NActCScore = (1-gamma)*(len(trecLowImportCats & runLowImportCats) / len(trecLowImportCats | runLowImportCats))\n",
    "        \n",
    "        worth = 0.0\n",
    "        if trecPriority == \"High\" or trecPriority == \"Critical\":\n",
    "            # calculate highImportWorth\n",
    "            if runPriority == \"High\" or runPriority == \"Critical\":\n",
    "                worth = var_alpha + ((1 - var_alpha) * (ActCScore+NActCScore))\n",
    "                numConsecutiveFalseAlerts = 0\n",
    "            else:\n",
    "                worth = -1\n",
    "            \n",
    "            #print(eventId+\" \"+tweetId+\" (High) \"+str(worth))\n",
    "            \n",
    "            \n",
    "            totalHighImportWorth = totalHighImportWorth + worth\n",
    "        else:\n",
    "            if runPriority == \"High\" or runPriority == \"Critical\":\n",
    "                worth = max(-math.log10((numConsecutiveFalseAlerts/2)+1),-1)\n",
    "                numConsecutiveFalseAlerts = numConsecutiveFalseAlerts + 1\n",
    "            else:\n",
    "                worth = ActCScore+NActCScore\n",
    "            \n",
    "            #print(eventId+\" \"+tweetId+\" (Low) \"+str(worth)+\" (\"+str(numConsecutiveFalseAlerts)+\")\")\n",
    "            \n",
    "            totalLowImportWorth = totalLowImportWorth + worth\n",
    "            \n",
    "        AccumulatedAlertWorth = AccumulatedAlertWorth + worth\n",
    "        \n",
    "    \n",
    "totalHighImportWorth = totalHighImportWorth / countHighCriticalImport\n",
    "totalLowImportWorth = totalLowImportWorth / countLowMediumImport\n",
    "AccumulatedAlertWorth = totalHighImportWorth + totalLowImportWorth\n",
    "        \n",
    "AccumulatedAlertWorth = (AccumulatedAlertWorth/2)\n",
    "totalHighImportWorth = totalHighImportWorth\n",
    "totalLowImportWorth = totalLowImportWorth\n",
    "\n",
    "print(\"High Importance Alert Worth: \"+str(totalHighImportWorth))\n",
    "print(\"Low Importance Alert Worth: \"+str(totalLowImportWorth))\n",
    "print(\"Accumulated Alert Worth: \"+str(AccumulatedAlertWorth))\n",
    "       \n",
    "resultsFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "resultsFile.write(\"EVALUATON: Alertng Performance\"+\"\\n\")\n",
    "resultsFile.write(\"Overall performance\"+\"\\n\")\n",
    "resultsFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "resultsFile.write(\"> High Importance Alert Worth:\"+\"\\t\"+str(totalHighImportWorth)+\"\\n\")\n",
    "resultsFile.write(\"> Low Importance Alert Worth:\"+\"\\t\"+str(totalLowImportWorth)+\"\\n\")\n",
    "resultsFile.write(\"> Accumulated Alert Worth:\"+\"\\t\"+str(AccumulatedAlertWorth)+\"\\n\")\n",
    "resultsFile.write(\"\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# TREC-IS 2020-A\n",
    "# Priority-Centric Discounted Cumulative Gain\n",
    "# --------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def calc_dcg(scores, at_k=100):\n",
    "    position = 1\n",
    "    accumulator = 0.0\n",
    "    for score in scores[:at_k]:\n",
    "\n",
    "        numerator = 2 ** score - 1\n",
    "        denom = np.log2(position + 1)\n",
    "\n",
    "        accumulator += numerator / denom\n",
    "        position += 1\n",
    "\n",
    "    return accumulator\n",
    "\n",
    "priority_map = {\n",
    "    \"Low\": 1,\n",
    "    \"Medium\": 2,\n",
    "    \"High\": 3,\n",
    "    \"Critical\": 4,\n",
    "}\n",
    "\n",
    "at_k = 100\n",
    "\n",
    "tweetId2TRECPriorityCategory_score = {\n",
    "    k:priority_map[v] for k,v in tweetId2TRECPriorityCategory.items()\n",
    "}\n",
    "tweetId2TRECPriorityCategory_scores_sorted = sorted(\n",
    "    tweetId2TRECPriorityCategory_score.values(),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "best_dcg_per_event = {}\n",
    "for event, rel_tweets in event2tweetIds.items():\n",
    "    print(event)\n",
    "    \n",
    "    tweetId2TRECPriorityCategory_scores_sorted = sorted(\n",
    "        [tweetId2TRECPriorityCategory_score[x] for x in rel_tweets],\n",
    "        reverse=True\n",
    "    )\n",
    "    ideal_dcg = calc_dcg(tweetId2TRECPriorityCategory_scores_sorted, at_k)\n",
    "    print(\"\\tBest DCG:\", ideal_dcg)\n",
    "    best_dcg_per_event[event] = ideal_dcg\n",
    "    \n",
    "print(\"Mean:\", np.mean(list(best_dcg_per_event.values())))\n",
    "print()\n",
    "\n",
    "# Code below calculates the DCG for a system's \n",
    "#  ranked priority tweets. We have to do some \n",
    "#  sampling here to break ties among tweets with\n",
    "#  the same priority scores.\n",
    "\n",
    "# Build a dataframe from the system's provided\n",
    "#  priority scores, so we can identify what the\n",
    "#  top-most priorities are and get a count of\n",
    "#  the number of tweets in each priority bin.\n",
    "priority_df = pd.DataFrame(\n",
    "    [(k, priority_map[v]) for k, v in tweetId2RunPriorityScoreNorm.items()],\n",
    "    columns=[\"tweet_id\", \"priority\"]\n",
    ")\n",
    "\n",
    "# Build metrics for each event\n",
    "system_dcg_per_event = {}\n",
    "for event, rel_tweets in event2tweetIds.items():\n",
    "    print(\"Event:\", event)\n",
    "    local_priority_df = priority_df[priority_df[\"tweet_id\"].isin(set(rel_tweets))]\n",
    "    \n",
    "    unique_scores = local_priority_df[\"priority\"].value_counts()\n",
    "    \n",
    "    # Find the top priority scores that would be included\n",
    "    #  in the necessary at_k values.\n",
    "    total = 0\n",
    "    top_keys = []\n",
    "    candidates = {}\n",
    "    for top in sorted(unique_scores.index, reverse=True):\n",
    "\n",
    "        # We store this key, so we can go back and shuffle\n",
    "        #. tweets with this score.\n",
    "        top_keys.append(top)\n",
    "        local_restricted_df = local_priority_df[local_priority_df[\"priority\"] == top]\n",
    "        candidates[top] = list(local_restricted_df[\"tweet_id\"])\n",
    "\n",
    "        total += local_restricted_df.shape[0]\n",
    "\n",
    "        # Once we have enough samples, stop.\n",
    "        if ( total > at_k ):\n",
    "            break\n",
    "\n",
    "    # Now we generate distribution over the DCG for this\n",
    "    #  system and do this a number of times to remove\n",
    "    #  dependence on our selection of the top k tweets\n",
    "    random_dcgs = []\n",
    "    for i in range(100):\n",
    "\n",
    "        local_tweet_ids = []\n",
    "        for top in top_keys:\n",
    "            this_top_tweets = candidates[top][:]\n",
    "            np.random.shuffle(this_top_tweets)\n",
    "\n",
    "            needed = at_k - len(local_tweet_ids)\n",
    "            local_tweet_ids.extend(this_top_tweets[:needed])\n",
    "\n",
    "        local_scores = [tweetId2TRECPriorityCategory_score[x] for x in local_tweet_ids]\n",
    "\n",
    "        random_dcgs.append(calc_dcg(local_scores))\n",
    "\n",
    "    system_dcg = np.mean(random_dcgs)\n",
    "\n",
    "    system_ndcg_ = system_dcg / best_dcg_per_event[event]\n",
    "    print(\"\\tnDCG:\", system_ndcg_)\n",
    "    system_dcg_per_event[event] = system_ndcg_\n",
    "    \n",
    "print()\n",
    "system_ndcg_micro = np.mean(list(system_dcg_per_event.values()))\n",
    "print(\"System Event-Micro nDCG:\", system_ndcg_micro)\n",
    "\n",
    "resultsFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "resultsFile.write(\"EVALUATON: nDCG and Priority\"+\"\\n\")\n",
    "resultsFile.write(\"Overall performance\"+\"\\n\")\n",
    "resultsFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "resultsFile.write(\"> nDCG:\"+\"\\t\"+str(system_ndcg_micro)+\"\\n\")\n",
    "resultsFile.write(\"\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# TREC-IS 2020-A\n",
    "# Task 1\n",
    "# Information Type Categorization\n",
    "# Overall performance\n",
    "# --------------------------------------------------\n",
    "# Average performance over information types\n",
    "# Macro averaged (information types have equal weight)\n",
    "# Does not average across events (larger events have more impact)\n",
    "# Positive class is the target class\n",
    "# Precision, recall and F1 only consider the positive class\n",
    "# Accuracy is an overall metric\n",
    "# We report performance for all categories, high importance categories and low importance categories\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "avgPrecision = 0.0\n",
    "avgRecall = 0.0\n",
    "avgF1 = 0.0\n",
    "avgAccuracy = 0.0\n",
    "\n",
    "avgPrecisionHigh = 0.0\n",
    "avgRecallHigh = 0.0\n",
    "avgF1High = 0.0\n",
    "avgAccuracyHigh = 0.0\n",
    "\n",
    "avgPrecisionLow = 0.0\n",
    "avgRecallLow = 0.0\n",
    "avgF1Low = 0.0\n",
    "avgAccuracyLow = 0.0\n",
    "\n",
    "for categoryId in informationTypes2Index.keys():\n",
    "    categoryPrecision = precision_score(category2GroundTruth[categoryId], category2Predicted[categoryId], average='binary')\n",
    "    categoryRecall = recall_score(category2GroundTruth[categoryId], category2Predicted[categoryId], average='binary')\n",
    "    categoryF1 = f1_score(category2GroundTruth[categoryId], category2Predicted[categoryId], average='binary')\n",
    "    categoryAccuracy = accuracy_score(category2GroundTruth[categoryId], category2Predicted[categoryId])\n",
    "    \n",
    "    avgPrecision = avgPrecision + categoryPrecision\n",
    "    avgRecall = avgRecall + categoryRecall\n",
    "    avgF1 = avgF1 + categoryF1\n",
    "    avgAccuracy = avgAccuracy + categoryAccuracy\n",
    "    \n",
    "    if any(categoryId in s for s in highImportCategories):\n",
    "        avgPrecisionHigh = avgPrecisionHigh + categoryPrecision\n",
    "        avgRecallHigh = avgRecallHigh + categoryRecall\n",
    "        avgF1High = avgF1High + categoryF1\n",
    "        avgAccuracyHigh = avgAccuracyHigh + categoryAccuracy\n",
    "    else:\n",
    "        avgPrecisionLow = avgPrecisionLow + categoryPrecision\n",
    "        avgRecallLow = avgRecallLow + categoryRecall\n",
    "        avgF1Low = avgF1Low + categoryF1\n",
    "        avgAccuracyLow = avgAccuracyLow + categoryAccuracy\n",
    "\n",
    "numInformationTypes = len(informationTypes2Index)\n",
    "numHighInformationTypes = len(highImportCategories)\n",
    "numLowInformationTypes = numInformationTypes - numHighInformationTypes\n",
    "        \n",
    "print(\"Information Type Precision (positive class, multi-type, macro): \"+str(avgPrecision/numInformationTypes))\n",
    "print(\"Information Type Recall (positive class, multi-type, macro): \"+str(avgRecall/numInformationTypes))\n",
    "print(\"Information Type F1 (positive class, multi-type, macro): \"+str(avgF1/numInformationTypes))\n",
    "print(\"Information Type Accuracy (overall, multi-type, macro): \"+str(avgAccuracy/numInformationTypes))\n",
    "\n",
    "print(\"High Importance Information Type Precision (positive class, multi-type, macro): \"+str(avgPrecisionHigh/numHighInformationTypes))\n",
    "print(\"High Importance Information Type Recall (positive class, multi-type, macro): \"+str(avgRecallHigh/numHighInformationTypes))\n",
    "print(\"High Importance Information Type F1 (positive class, multi-type, macro): \"+str(avgF1High/numHighInformationTypes))\n",
    "print(\"High Importance Information Type Accuracy (overall, multi-type, macro): \"+str(avgAccuracyHigh/numHighInformationTypes))\n",
    "\n",
    "print(\"Low Importance Information Type Precision (positive class, multi-type, macro): \"+str(avgPrecisionLow/numLowInformationTypes))\n",
    "print(\"Low Importance Information Type Recall (positive class, multi-type, macro): \"+str(avgRecallLow/numLowInformationTypes))\n",
    "print(\"Low Importance Information Type F1 (positive class, multi-type, macro): \"+str(avgF1Low/numLowInformationTypes))\n",
    "print(\"Low Importance Information Type Accuracy (overall, multi-type, macro): \"+str(avgAccuracyLow/numLowInformationTypes))\n",
    "\n",
    "resultsFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "resultsFile.write(\"EVALUATON: Information Type Categorization\"+\"\\n\")\n",
    "resultsFile.write(\"Overall performance\"+\"\\n\")\n",
    "resultsFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "resultsFile.write(\"> Information Type Precision (positive class, multi-type, macro):\"+\"\\t\"+str(avgPrecision/len(informationTypes2Index))+\"\\n\")\n",
    "resultsFile.write(\"> Information Type Recall (positive class, multi-type, macro):\"+\"\\t\"+str(avgRecall/len(informationTypes2Index))+\"\\n\")\n",
    "resultsFile.write(\"> Information Type F1 (positive class, multi-type, macro):\"+\"\\t\"+str(avgF1/len(informationTypes2Index))+\"\\n\")\n",
    "resultsFile.write(\"> Information Type Accuracy (overall, multi-type, macro):\"+\"\\t\"+str(avgAccuracy/len(informationTypes2Index))+\"\\n\")\n",
    "resultsFile.write(\"> High Importance Information Type Precision (positive class, multi-type, macro):\"+\"\\t\"+str(avgPrecisionHigh/numHighInformationTypes)+\"\\n\")\n",
    "resultsFile.write(\"> High Importance Information Type Recall (positive class, multi-type, macro):\"+\"\\t\"+str(avgRecallHigh/numHighInformationTypes)+\"\\n\")\n",
    "resultsFile.write(\"> High Importance Information Type F1 (positive class, multi-type, macro):\"+\"\\t\"+str(avgF1High/numHighInformationTypes)+\"\\n\")\n",
    "resultsFile.write(\"> High Importance Information Type Accuracy (overall, multi-type, macro):\"+\"\\t\"+str(avgAccuracyHigh/numHighInformationTypes)+\"\\n\")\n",
    "resultsFile.write(\"> Low Importance Information Type Precision (positive class, multi-type, macro):\"+\"\\t\"+str(avgPrecisionLow/numLowInformationTypes)+\"\\n\")\n",
    "resultsFile.write(\"> Low Importance Information Type Recall (positive class, multi-type, macro):\"+\"\\t\"+str(avgRecallLow/numLowInformationTypes)+\"\\n\")\n",
    "resultsFile.write(\"> Low Importance Information Type F1 (positive class, multi-type, macro):\"+\"\\t\"+str(avgF1Low/numLowInformationTypes)+\"\\n\")\n",
    "resultsFile.write(\"> Low Importance Information Type Accuracy (overall, multi-type, macro):\"+\"\\t\"+str(avgAccuracyLow/numLowInformationTypes)+\"\\n\")\n",
    "resultsFile.write(\"\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# TREC-IS 2020-A\n",
    "# Task 1\n",
    "# Information Type Categorization\n",
    "# Per Information Type Performance\n",
    "# --------------------------------------------------\n",
    "# Per Category Classification Performance with confusion matrices\n",
    "# Performance on the target class is what we care about here, \n",
    "# primaraly with respect to recall, as we want the user to \n",
    "# see all of the information for a given category. A small\n",
    "# amount of noise being added to the feed is an acceptable\n",
    "# cost for good recall.\n",
    "#\n",
    "# Does not average across events (larger events have more impact)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "perTopicFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "perTopicFile.write(\"EVALUATON: Information Type Categorization (Multi-type)\"+\"\\n\")\n",
    "perTopicFile.write(\"Per Information Type Performance\"+\"\\n\")\n",
    "perTopicFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "\n",
    "for categoryId in informationTypes2Index.keys():\n",
    "    target_names = ['Other Classes', categoryId]\n",
    "    print(categoryId)\n",
    "    print(classification_report(category2GroundTruth[categoryId], category2Predicted[categoryId], target_names=target_names))\n",
    "\n",
    "\n",
    "    perTopicFile.write(categoryId+\"\\n\")\n",
    "    perTopicFile.write(classification_report(category2GroundTruth[categoryId], category2Predicted[categoryId], target_names=target_names)+\"\\n\")\n",
    "    perTopicFile.write(\"\"+\"\\n\")\n",
    "\n",
    "perTopicFile.write(\"\"+\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# TREC-IS 2020-A\n",
    "# Task 1\n",
    "# Information Type Categorization\n",
    "# Per Information Type F1 Graph\n",
    "# --------------------------------------------------\n",
    "# Per Category Classification Performance\n",
    "# F1 scores for each information type, graphed\n",
    "# Does not average across events (larger events have more impact)\n",
    "\n",
    "\n",
    "\n",
    "N = len(informationTypes2Index)\n",
    "ind = np.arange(N)\n",
    "\n",
    "scoresPerCategoryF1 = []\n",
    "categoryLabels = []\n",
    "for categoryId in informationTypes2Index.keys():\n",
    "    scoresPerCategoryF1.append(f1_score(category2GroundTruth[categoryId], category2Predicted[categoryId], average='binary'))\n",
    "    categoryLabels.append(categoryId)\n",
    "    \n",
    "width = 0.90       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "p1 = plt.bar(ind, scoresPerCategoryF1, width)\n",
    "\n",
    "plt.ylabel('F1 Scores')\n",
    "plt.title('F1 Scores by Information Type')\n",
    "plt.xticks(ind, categoryLabels, rotation='vertical')\n",
    "plt.yticks(np.arange(0, 1, 0.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# TREC-IS 2020-A\n",
    "# Task 1\n",
    "# Information Type Categorization\n",
    "# Per Event Performance\n",
    "# --------------------------------------------------\n",
    "# Categorization performance for each event\n",
    "# Precision, recall and F1 only consider the positive class\n",
    "# Accuracy is an overall metric\n",
    "# We report performance for all categories, high importance categories and low importance categories\n",
    "# Macro average (categories have equal weight)\n",
    "\n",
    "perEventFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "perEventFile.write(\"EVALUATON: Information Type Categorization (Multi-type)\"+\"\\n\")\n",
    "perEventFile.write(\"Per Event Performance\"+\"\\n\")\n",
    "perEventFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "\n",
    "for eventId in eventIdentifiers:\n",
    "    tavgPrecision = 0.0\n",
    "    tavgRecall = 0.0\n",
    "    tavgF1 = 0.0\n",
    "    tavgAccuracy = 0.0\n",
    "    \n",
    "    tavgPrecisionHigh = 0.0\n",
    "    tavgRecallHigh = 0.0\n",
    "    tavgF1High = 0.0\n",
    "    tavgAccuracyHigh = 0.0\n",
    "\n",
    "    tavgPrecisionLow = 0.0\n",
    "    tavgRecallLow = 0.0\n",
    "    tavgF1Low = 0.0\n",
    "    tavgAccuracyLow = 0.0\n",
    "\n",
    "    for categoryId in informationTypes2Index.keys():\n",
    "        \n",
    "        categoryPrecision = precision_score(event2groundtruth[eventId].get(categoryId), event2prediction[eventId].get(categoryId), average='binary')\n",
    "        categoryRecall = recall_score(event2groundtruth[eventId].get(categoryId), event2prediction[eventId].get(categoryId), average='binary')\n",
    "        categoryF1 = f1_score(event2groundtruth[eventId].get(categoryId), event2prediction[eventId].get(categoryId), average='binary')\n",
    "        categoryAccuracy = accuracy_score(event2groundtruth[eventId].get(categoryId), event2prediction[eventId].get(categoryId))\n",
    "        \n",
    "        tavgPrecision = tavgPrecision + categoryPrecision\n",
    "        tavgRecall = tavgRecall + categoryRecall\n",
    "        tavgF1 = tavgF1 + categoryF1\n",
    "        tavgAccuracy = tavgAccuracy + categoryAccuracy\n",
    "        \n",
    "        if any(categoryId in s for s in highImportCategories):\n",
    "            tavgPrecisionHigh = tavgPrecisionHigh + categoryPrecision\n",
    "            tavgRecallHigh = tavgRecallHigh + categoryRecall\n",
    "            tavgF1High = tavgF1High + categoryF1\n",
    "            tavgAccuracyHigh = tavgAccuracyHigh + categoryAccuracy\n",
    "        else:\n",
    "            tavgPrecisionLow = tavgPrecisionLow + categoryPrecision\n",
    "            tavgRecallLow = tavgRecallLow + categoryRecall\n",
    "            tavgF1Low = tavgF1Low + categoryF1\n",
    "            tavgAccuracyLow = tavgAccuracyLow + categoryAccuracy\n",
    "        \n",
    "    print(eventId)\n",
    "    print(\"  Information Type Precision (positive class, multi-type, macro): \"+str(tavgPrecision/len(informationTypes2Index)))\n",
    "    print(\"  Information Type Recall (positive class, multi-type, macro): \"+str(tavgRecall/len(informationTypes2Index)))\n",
    "    print(\"  Information Type F1 (positive class, multi-type, macro): \"+str(tavgF1/len(informationTypes2Index)))\n",
    "    print(\"  Information Type Accuracy (overall, multi-type, macro): \"+str(tavgAccuracy/len(informationTypes2Index)))\n",
    "    print(\"  High Importance Information Type Precision (positive class, multi-type, macro): \"+str(tavgPrecisionHigh/numHighInformationTypes))\n",
    "    print(\"  High Importance Information Type Recall (positive class, multi-type, macro): \"+str(tavgRecallHigh/numHighInformationTypes))\n",
    "    print(\"  High Importance Information Type F1 (positive class, multi-type, macro): \"+str(tavgF1High/numHighInformationTypes))\n",
    "    print(\"  High Importance Information Type Accuracy (overall, multi-type, macro): \"+str(tavgAccuracyHigh/numHighInformationTypes))\n",
    "    print(\"  Low Importance Information Type Precision (positive class, multi-type, macro): \"+str(tavgPrecisionLow/numLowInformationTypes))\n",
    "    print(\"  Low Importance Information Type Recall (positive class, multi-type, macro): \"+str(tavgRecallLow/numLowInformationTypes))\n",
    "    print(\"  Low Importance Information Type F1 (positive class, multi-type, macro): \"+str(tavgF1Low/numLowInformationTypes))\n",
    "    print(\"  Low Importance Information Type Accuracy (overall, multi-type, macro): \"+str(tavgAccuracyLow/numLowInformationTypes))\n",
    "    print(\"\")\n",
    "    \n",
    "    perEventFile.write(eventId+\"\\n\")\n",
    "    perEventFile.write(\"  Information Type Precision (positive class, multi-type, macro): \"+str(tavgPrecision/len(informationTypes2Index))+\"\\n\")\n",
    "    perEventFile.write(\"  Information Type Recall (positive class, multi-type, macro): \"+str(tavgRecall/len(informationTypes2Index))+\"\\n\")\n",
    "    perEventFile.write(\"  Information Type F1 (positive class, multi-type, macro): \"+str(tavgF1/len(informationTypes2Index))+\"\\n\")\n",
    "    perEventFile.write(\"  Information Type Accuracy (overall, multi-type, macro): \"+str(tavgAccuracy/len(informationTypes2Index))+\"\\n\")\n",
    "    perEventFile.write(\"  High Importance Information Type Precision (positive class, multi-type, macro): \"+str(tavgPrecisionHigh/numHighInformationTypes)+\"\\n\")\n",
    "    perEventFile.write(\"  High Importance Information Type Recall (positive class, multi-type, macro): \"+str(tavgRecallHigh/numHighInformationTypes)+\"\\n\")\n",
    "    perEventFile.write(\"  High Importance Information Type F1 (positive class, multi-type, macro): \"+str(tavgF1High/numHighInformationTypes)+\"\\n\")\n",
    "    perEventFile.write(\"  High Importance Information Type Accuracy (overall, multi-type, macro): \"+str(tavgAccuracyHigh/numHighInformationTypes)+\"\\n\")\n",
    "    perEventFile.write(\"  Low Importance Information Type Precision (positive class, multi-type, macro): \"+str(tavgPrecisionLow/numLowInformationTypes)+\"\\n\")\n",
    "    perEventFile.write(\"  Low Importance Information Type Recall (positive class, multi-type, macro): \"+str(tavgRecallLow/numLowInformationTypes)+\"\\n\")\n",
    "    perEventFile.write(\"  Low Importance Information Type F1 (positive class, multi-type, macro): \"+str(tavgF1Low/numLowInformationTypes)+\"\\n\")\n",
    "    perEventFile.write(\"  Low Importance Information Type Accuracy (overall, multi-type, macro): \"+str(tavgAccuracyLow/numLowInformationTypes)+\"\\n\")\n",
    "    perEventFile.write(\"\\n\")\n",
    "    \n",
    "perEventFile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# TREC-IS 2020-A\n",
    "# Task 1\n",
    "# Information Type Categorization\n",
    "# Per Event F1 Graph\n",
    "# --------------------------------------------------\n",
    "# Multi-type (1 vs All): Tweets have multiple information types, aim: predict all of them\n",
    "# Macro average (categories have equal weight)\n",
    "\n",
    "N = len(eventIdentifiers)\n",
    "ind = np.arange(N)\n",
    "\n",
    "scoresPerEventF1 = []\n",
    "for eventId in eventIdentifiers:\n",
    "    avgF1_ = 0.0\n",
    "    \n",
    "    for categoryId in informationTypes2Index.keys():\n",
    "        avgF1_ = avgF1_ + f1_score(event2groundtruth[eventId].get(categoryId), event2prediction[eventId].get(categoryId), average='binary')\n",
    "        \n",
    "    scoresPerEventF1.append(avgF1_/len(informationTypes2Index))\n",
    "    \n",
    "width = 0.90       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "p1 = plt.bar(ind, scoresPerEventF1, width)\n",
    "\n",
    "plt.ylabel('F1 Scores')\n",
    "plt.title('F1 Category Scores by Event')\n",
    "plt.xticks(ind, eventIdentifiers, rotation='vertical')\n",
    "plt.yticks(np.arange(0, 1, 0.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# TREC-IS 2020-A\n",
    "# Task 1\n",
    "# Information Priority Level\n",
    "# Overall Performance\n",
    "# --------------------------------------------------\n",
    "# How divergent is the system from the human priority labels?\n",
    "# F1 performance over information types, higher is better\n",
    "# Macro average (categories have equal weight)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "priorityAvgf1 = 0.0;\n",
    "priorityAvgf1High = 0.0;\n",
    "priorityAvgf1Low = 0.0;\n",
    "for categoryId in informationTypes2Index.keys():\n",
    "    groundTruthPriorities = category2GroundTruthPriority[categoryId]\n",
    "    predictedPriorities = category2PredictedPriority[categoryId]\n",
    "\n",
    "    f1 = f1_score(groundTruthPriorities, predictedPriorities, average='macro')\n",
    "    priorityAvgf1 = priorityAvgf1 + f1;\n",
    "    \n",
    "    if any(categoryId in s for s in highImportCategories):\n",
    "        priorityAvgf1High = priorityAvgf1High + f1\n",
    "    else:\n",
    "        priorityAvgf1Low = priorityAvgf1Low + f1\n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Priority Label Prediction (F1, macro): \"+str(priorityAvgf1/len(informationTypes2Index)))\n",
    "print(\"Priority Label Prediction High Importance (F1, macro): \"+str(priorityAvgf1High/numHighInformationTypes))\n",
    "print(\"Priority Label Prediction Low Importance (F1, macro): \"+str(priorityAvgf1Low/numLowInformationTypes))\n",
    "    \n",
    "resultsFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "resultsFile.write(\"EVALUATON: Information Priority Level\"+\"\\n\")\n",
    "resultsFile.write(\"Overall Performance\"+\"\\n\")\n",
    "resultsFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "resultsFile.write(\"> Priority Label Prediction (F1, macro): \"+str(priorityAvgf1/len(informationTypes2Index))+\"\\n\")\n",
    "resultsFile.write(\"> Priority Label Prediction High Importance (F1, macro): \"+str(priorityAvgf1High/numHighInformationTypes)+\"\\n\")\n",
    "resultsFile.write(\"> Priority Label Prediction Low Importance (F1, macro): \"+str(priorityAvgf1Low/numLowInformationTypes)+\"\\n\")\n",
    "resultsFile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# TREC-IS 2020-A\n",
    "# Task 1\n",
    "# Information Priority Level\n",
    "# Per Information Type Performance\n",
    "# --------------------------------------------------\n",
    "# F1 per information type (macro averaged), higher is better\n",
    "# Macro average (categories have equal weight)\n",
    "\n",
    "N = len(informationTypes2Index)\n",
    "ind = np.arange(N)\n",
    "\n",
    "priorityCatF1Values = []\n",
    "categoryLabels = []\n",
    "for categoryId in informationTypes2Index.keys():\n",
    "    groundTruthPriorities = category2GroundTruthPriority[categoryId]\n",
    "    predictedPriorities = category2PredictedPriority[categoryId]\n",
    "    priorityCatF1 = f1_score(groundTruthPriorities, predictedPriorities, average='macro')\n",
    "    categoryLabels.append(categoryId)\n",
    "    priorityCatF1Values.append(priorityCatF1);\n",
    "    \n",
    "width = 0.90       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "p1 = plt.bar(ind, priorityCatF1Values, width)\n",
    "\n",
    "plt.ylabel('Priorty Label Prediction F1 (higher is better)')\n",
    "plt.title('Priorty Label Prediction F1 Per Information Type')\n",
    "plt.xticks(ind, categoryLabels, rotation='vertical')\n",
    "plt.yticks(np.arange(0, 1, 0.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the evaluation table row in latex\n",
    "print(\"Run & NDCG & AW-H & AW-A & CF1-H & CF1-A & CAcc & PErr-H & PErr-A \\\\\\\\\")\n",
    "\n",
    "resultLine = (str.format('{0:.4f}', system_ndcg_micro)+\n",
    "     \" & \"+\n",
    "     str.format('{0:.4f}', totalHighImportWorth)+\n",
    "     \" & \"+\n",
    "     str.format('{0:.4f}', AccumulatedAlertWorth)+\n",
    "     \" & \"+\n",
    "     str.format('{0:.4f}',avgF1High/numHighInformationTypes)+\n",
    "     \" & \"+\n",
    "     str.format('{0:.4f}',avgF1/numInformationTypes)+\n",
    "     \" & \"+\n",
    "     str.format('{0:.4f}',avgAccuracy/numInformationTypes)+\n",
    "     \" & \"+\n",
    "     str.format('{0:.4f}',priorityAvgf1High/numHighInformationTypes)+\n",
    "     \" & \"+\n",
    "     str.format('{0:.4f}',priorityAvgf1/len(informationTypes2Index))+\n",
    "     \" \\\\\\\\\")\n",
    "\n",
    "print(runName+\" & \"+resultLine)\n",
    "\n",
    "resultsFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "resultsFile.write(\"LATEX\"+\"\\n\")\n",
    "resultsFile.write(\"--------------------------------------------------\"+\"\\n\")\n",
    "resultsFile.write(runName+\" & \"+resultLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done\n",
    "resultsFile.close() \n",
    "perTopicFile.close()\n",
    "perEventFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
